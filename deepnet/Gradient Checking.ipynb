{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient checking of Layers\n",
    "\n",
    "Verify the correctness of implementation using Gradient checks provided in CS231 2nd assignment.\n",
    "\n",
    "1. **Probably Wrong**: relative error > 1e-2 \n",
    "2. **Something not right** :1e-2 > relative error > 1e-4 \n",
    "3. **Okay for objectives with kinks**: 1e-4 > relative error, if no kinks then too high\n",
    "4. **Most likely Right**: relative error < 1e-7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import *\n",
    "from loss import SoftmaxLoss\n",
    "from nnet import NeuralNet\n",
    "from solver import sgd,sgd_momentum,adam\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    ix = it.multi_index\n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "\n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "\n",
    "    it.iternext()\n",
    "  return grad\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "  \"\"\"\n",
    "  a naive implementation of numerical gradient of f at x\n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\"\n",
    "\n",
    "  fx = f(x) # evaluate function value at original point\n",
    "\n",
    "  grad = np.zeros_like(x)\n",
    "  # iterate over all indexes in x\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    # evaluate function at x+h\n",
    "    ix = it.multi_index\n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x) # evalute f(x + h)\n",
    "    x[ix] = oldval - h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] = oldval # restore\n",
    "\n",
    "    # compute the partial derivative with centered formula\n",
    "    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "    if verbose:\n",
    "      print(ix, grad[ix])\n",
    "    it.iternext() # step to next dimension\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "\n",
    "Perform numerical grdient checking for verifying the implementation of convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "The difference of correct_out and out should be around 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass of Conv Layer\n",
      "Difference:  2.21214764967e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "c_layer = Conv((3,4,4),n_filter=3,h_filter=4,w_filter=4,stride=2,padding=1)\n",
    "c_layer.W = w\n",
    "c_layer.b = b.reshape(-1,1)\n",
    "\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "out = c_layer.forward(x)\n",
    "\n",
    "error = rel_error(out,correct_out)\n",
    "print(\"Testing forward pass of Conv Layer\")\n",
    "print(\"Difference: \",error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "The errors for gradients should be around 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of Conv Layer\n",
      "dX error:  6.30285589596e-09\n",
      "dW error:  3.66468373932e-10\n",
      "db error:  6.8390384471e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,).reshape(-1,1)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "\n",
    "c_layer = Conv((3,5,5),n_filter=2,h_filter=3,w_filter=3,stride=1,padding=1)\n",
    "c_layer.W = w\n",
    "c_layer.b = b\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: c_layer.forward(x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: c_layer.forward(x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: c_layer.forward(x), b, dout)\n",
    "\n",
    "out = c_layer.forward(x)\n",
    "dx,grads = c_layer.backward(dout)\n",
    "dw,db = grads\n",
    "\n",
    "print(\"Testing backward pass of Conv Layer\")\n",
    "print(\"dX error: \",rel_error(dx,dx_num))\n",
    "print(\"dW error: \",rel_error(dw,dw_num))\n",
    "print(\"db error: \",rel_error(db,db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpool Layer\n",
    "\n",
    "Perform gradient check for maxpool layer and verify correctness of its implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "Difference should be around 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.16666651573e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "\n",
    "pool = Maxpool((3,4,4),size=2,stride=2)\n",
    "\n",
    "out = pool.forward(x,)\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "\n",
    "Error should be around 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bacward pass of Maxpool layer\n",
      "dX error:  3.27561819731e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "\n",
    "pool = Maxpool((2,8,8),size=2,stride=2)\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: pool.forward(x), x, dout)\n",
    "\n",
    "out = pool.forward(x)\n",
    "dx,_ = pool.backward(dout)\n",
    "\n",
    "print('Testing bacward pass of Maxpool layer')\n",
    "print('dX error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "Error should be around 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of ReLU layer\n",
      "dX error:  3.275621976e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 8, 8)\n",
    "\n",
    "r = ReLU()\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x:r.forward(x), x, dout)\n",
    "\n",
    "out = r.forward(x)\n",
    "dx,_ = r.backward(dout)\n",
    "\n",
    "print('Testing backward pass of ReLU layer')\n",
    "print('dX error: ',rel_error(dx,dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv-ReLU-MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  1.01339343448e-08\n",
      "dw error:  7.41563088659e-10\n",
      "db error:  7.51304173633e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,).reshape(-1,1)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "\n",
    "c = Conv((3,16,16),n_filter=3,h_filter=3,w_filter=3,stride=1,padding=1)\n",
    "c.W, c.b = w, b\n",
    "r = ReLU()\n",
    "m = Maxpool(c.out_dim,size=2,stride=2)\n",
    "\n",
    "def conv_relu_pool_forward(c,r,m,x):\n",
    "    c_out = c.forward(x)\n",
    "    r_out = r.forward(c_out)\n",
    "    m_out = m.forward(r_out)\n",
    "    return m_out\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: conv_relu_pool_forward(c,r,m,x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: conv_relu_pool_forward(c,r,m,x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: conv_relu_pool_forward(c,r,m,x), b, dout)\n",
    "\n",
    "m_dx,_ = m.backward(dout)\n",
    "r_dx,_ = r.backward(m_dx)\n",
    "dx,grads = c.backward(r_dx)\n",
    "dw,db = grads\n",
    "\n",
    "\n",
    "print('Testing conv_relu_pool')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.49834967  1.70660132  1.91485297]\n",
      " [ 3.25553199  3.5141327   3.77273342]]\n",
      "Testing fully connected forward pass:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim).reshape(1,-1)\n",
    "\n",
    "flat = Flatten()\n",
    "x = flat.forward(x)\n",
    "\n",
    "f = FullyConnected(120,3)\n",
    "f.W,f.b= w,b\n",
    "out = f.forward(x)\n",
    "\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "print(out)\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing fully connected forward pass:')\n",
    "print('difference: ', rel_error(out, correct_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fully connected backward pass:\n",
      "dx error:  2.89903091526e-09\n",
      "dw error:  1.32127575542e-09\n",
      "db error:  1.03150657456e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "flat = Flatten()\n",
    "x = flat.forward(x)\n",
    "\n",
    "f = FullyConnected(60,5)\n",
    "f.W,f.b= w,b\n",
    "\n",
    "dx_num = numerical_gradient_array(lambda x: f.forward(x), x, dout)\n",
    "dw_num = numerical_gradient_array(lambda w: f.forward(x), w, dout)\n",
    "db_num = numerical_gradient_array(lambda b: f.forward(x), b, dout)\n",
    "\n",
    "dx,grads= f.backward(dout)\n",
    "dw, db = grads\n",
    "# The error should be around 1e-10\n",
    "print('Testing fully connected backward pass:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SoftmaxLoss:\n",
      "loss:  2.30283790984\n",
      "dx error:  1.05396983612e-08\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: SoftmaxLoss(x,y)[0], x,verbose=False)\n",
    "loss,dx = SoftmaxLoss(x,y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('Testing SoftmaxLoss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
