{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superai.nn.layer.fc import FullyConnected\n",
    "from superai.nn.layer.activator import Activator\n",
    "from superai.nn.model.nnet import Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "from superai.nn.optimizer.optimizer import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration2  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration3  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration4  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration5  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration6  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration7  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration8  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration9  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration10  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration11  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration12  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration13  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration14  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration15  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration16  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration17  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration18  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration19  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration20  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration21  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration22  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration23  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration24  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration25  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration26  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration27  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration28  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration29  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration30  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration31  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration32  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration33  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration34  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration35  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration36  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration37  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration38  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration39  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration40  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration41  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration42  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration43  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration44  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration45  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration46  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration47  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration48  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration49  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration50  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration51  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration52  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration53  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration54  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration55  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration56  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration57  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration58  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration59  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration60  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration61  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration62  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration63  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration64  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration65  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration66  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration67  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration68  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration69  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration70  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration71  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration72  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration73  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration74  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration75  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration76  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration77  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration78  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration79  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration80  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration81  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration82  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration83  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration84  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration85  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration86  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration87  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration88  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration89  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration90  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration91  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration92  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration93  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration94  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration95  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration96  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration97  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration98  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration99  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "iteration100  cost:2.3866864108175503, accuracy:0.11519999999999997\n",
      "0.11519999999999997\n",
      "0.10119999999999996\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def calculate_accuracy(model, X, Y):\n",
    "    m, n = X.shape\n",
    "    Y_pre = model.predict(X)\n",
    "    Y_pre = Y_pre == np.max(Y_pre, axis=0, keepdims = True)\n",
    "    C = np.abs(Y_pre - Y)\n",
    "    error = np.sum(C) / 2\n",
    "    print(1 - error / n)\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_train, Y_train = load_mnist(path=\"MNIST\")\n",
    "X_train = X_train.transpose()\n",
    "Y_train = convert_to_one_hot(Y_train, 10)\n",
    "m, n = X_train.shape\n",
    "# normalization fc-layer inputs\n",
    "nn_input_min = np.min(X_train, axis=1, keepdims=True)\n",
    "nn_input_max = np.max(X_train, axis=1, keepdims=True)\n",
    "nn_input_final = ((X_train - nn_input_min) / (nn_input_max - nn_input_min + 1e-11))\n",
    "X_train = nn_input_final\n",
    "\n",
    "linearLayer1 = FullyConnected(m, 40)\n",
    "linearLayer1.layer = 1\n",
    "activator1 = Activator('tanh')\n",
    "linearLayer2 = FullyConnected(40, 20)\n",
    "linearLayer2.layer = 2\n",
    "activator2 = Activator('tanh')\n",
    "linearLayer3 = FullyConnected(20, 10)\n",
    "linearLayer3.layer = 3\n",
    "# outputLayer = NNActivator(linearLayer3, 'softmax', 10)\n",
    "\n",
    "model = Sequence([linearLayer1, activator1, linearLayer2, activator2, linearLayer3], learning_rate=0.02, iteration_count=100, lambd=0,\n",
    "                 use_mini_batch=False, mini_batch_size=64)\n",
    "adamOpt = Adam()\n",
    "adamOpt.run(model, X_train, Y_train)\n",
    "# model.fit(X_train, Y_train)\n",
    "\n",
    "X_test, Y_test = load_mnist(path=\"MNIST\", kind=\"t10k\")\n",
    "X_test = X_test.transpose()\n",
    "Y_test = convert_to_one_hot(Y_test, 10)\n",
    "\n",
    "calculate_accuracy(model, X_train, Y_train)\n",
    "calculate_accuracy(model, X_test, Y_test)\n",
    "\n",
    "print(\"over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
