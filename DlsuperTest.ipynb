{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superai.nn.layer.fc import FullyConnected\n",
    "from superai.nn.layer.activator import Activator\n",
    "from superai.nn.model.nnet import Sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "from superai.nn.optimizer.adam import Adam\n",
    "from superai.nn.optimizer.adagrad import AdaGrad\n",
    "from superai.nn.optimizer.momentum import Momentum\n",
    "from superai.nn.optimizer.rmsprop import RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1  cost:116.2827676710392, accuracy:0.11519999999999997\n",
      "iteration2  cost:112.47058715042519, accuracy:0.12031666666666663\n",
      "iteration3  cost:108.79053847268425, accuracy:0.12758333333333338\n",
      "iteration4  cost:105.23768394697568, accuracy:0.13460000000000005\n",
      "iteration5  cost:101.80718375854578, accuracy:0.1433833333333333\n",
      "iteration6  cost:98.49435553765815, accuracy:0.15346666666666664\n",
      "iteration7  cost:95.29471354227569, accuracy:0.1629166666666667\n",
      "iteration8  cost:92.20398638678604, accuracy:0.1729666666666667\n",
      "iteration9  cost:89.21811804357151, accuracy:0.1842666666666667\n",
      "iteration10  cost:86.33325838789607, accuracy:0.19518333333333338\n",
      "iteration11  cost:83.54574863569007, accuracy:0.20648333333333335\n",
      "iteration12  cost:80.85210531285549, accuracy:0.21756666666666669\n",
      "iteration13  cost:78.2490048262989, accuracy:0.22914999999999996\n",
      "iteration14  cost:75.73326959278681, accuracy:0.24076666666666668\n",
      "iteration15  cost:73.3018560092736, accuracy:0.25221666666666664\n",
      "iteration16  cost:70.95184419764897, accuracy:0.26444999999999996\n",
      "iteration17  cost:68.68042930613102, accuracy:0.2747333333333334\n",
      "iteration18  cost:66.4849141088989, accuracy:0.28498333333333337\n",
      "iteration19  cost:64.3627026582821, accuracy:0.2965333333333333\n",
      "iteration20  cost:62.31129477778187, accuracy:0.3071666666666667\n",
      "iteration21  cost:60.32828122290442, accuracy:0.3171666666666667\n",
      "iteration22  cost:58.41133937273785, accuracy:0.32715000000000005\n",
      "iteration23  cost:56.55822934567137, accuracy:0.33730000000000004\n",
      "iteration24  cost:54.76679045721986, accuracy:0.34691666666666665\n",
      "iteration25  cost:53.03493795714208, accuracy:0.35545000000000004\n",
      "iteration26  cost:51.36065999782181, accuracy:0.36429999999999996\n",
      "iteration27  cost:49.7420147971186, accuracy:0.3724166666666666\n",
      "iteration28  cost:48.17712796738335, accuracy:0.38075000000000003\n",
      "iteration29  cost:46.66418998872403, accuracy:0.3882333333333333\n",
      "iteration30  cost:45.20145380941109, accuracy:0.3952333333333333\n",
      "iteration31  cost:43.78723255992437, accuracy:0.40251666666666663\n",
      "iteration32  cost:42.41989736986512, accuracy:0.4097833333333334\n",
      "iteration33  cost:41.0978752790111, accuracy:0.4163\n",
      "iteration34  cost:39.81964723535059, accuracy:0.4220666666666667\n",
      "iteration35  cost:38.58374617411787, accuracy:0.42795000000000005\n",
      "iteration36  cost:37.38875517276141, accuracy:0.43435\n",
      "iteration37  cost:36.23330567747828, accuracy:0.4400666666666667\n",
      "iteration38  cost:35.116075797494304, accuracy:0.4454\n",
      "iteration39  cost:34.03578866369957, accuracy:0.4510666666666666\n",
      "iteration40  cost:32.99121084859084, accuracy:0.45596666666666663\n",
      "iteration41  cost:31.981150844748438, accuracy:0.46068333333333333\n",
      "iteration42  cost:31.00445759930016, accuracy:0.46519999999999995\n",
      "iteration43  cost:30.06001910201248, accuracy:0.46919999999999995\n",
      "iteration44  cost:29.146761024806736, accuracy:0.473\n",
      "iteration45  cost:28.263645410633192, accuracy:0.47735000000000005\n",
      "iteration46  cost:27.4096694097531, accuracy:0.4808\n",
      "iteration47  cost:26.583864061582435, accuracy:0.4848\n",
      "iteration48  cost:25.785293120343155, accuracy:0.4878166666666667\n",
      "iteration49  cost:25.013051922851414, accuracy:0.4914166666666666\n",
      "iteration50  cost:24.266266296848244, accuracy:0.49491666666666667\n",
      "iteration51  cost:23.544091508348195, accuracy:0.49796666666666667\n",
      "iteration52  cost:22.845711246546806, accuracy:0.5012666666666667\n",
      "iteration53  cost:22.170336644888195, accuracy:0.5039666666666667\n",
      "iteration54  cost:21.517205336951463, accuracy:0.5066999999999999\n",
      "iteration55  cost:20.885580545868173, accuracy:0.5093666666666667\n",
      "iteration56  cost:20.274750206034344, accuracy:0.5115333333333334\n",
      "iteration57  cost:19.68402611592861, accuracy:0.5139666666666667\n",
      "iteration58  cost:19.112743120894464, accuracy:0.5162833333333333\n",
      "iteration59  cost:18.560258324788286, accuracy:0.5180166666666667\n",
      "iteration60  cost:18.02595032943693, accuracy:0.5199166666666667\n",
      "iteration61  cost:17.509218500888885, accuracy:0.5217666666666667\n",
      "iteration62  cost:17.009482261481345, accuracy:0.5231833333333333\n",
      "iteration63  cost:16.52618040678269, accuracy:0.5246333333333333\n",
      "iteration64  cost:16.058770446504994, accuracy:0.5258333333333334\n",
      "iteration65  cost:15.606727968515433, accuracy:0.5266500000000001\n",
      "iteration66  cost:15.169546025107994, accuracy:0.5277000000000001\n",
      "iteration67  cost:14.746734540728246, accuracy:0.5286\n",
      "iteration68  cost:14.337819740374252, accuracy:0.5290833333333333\n",
      "iteration69  cost:13.942343597925518, accuracy:0.5295666666666667\n",
      "iteration70  cost:13.559863303679943, accuracy:0.5298166666666666\n",
      "iteration71  cost:13.18995075040548, accuracy:0.5302166666666667\n",
      "iteration72  cost:12.832192037239036, accuracy:0.5304166666666666\n",
      "iteration73  cost:12.486186990790056, accuracy:0.5304333333333333\n",
      "iteration74  cost:12.15154870283004, accuracy:0.5297833333333333\n",
      "iteration75  cost:11.827903083972323, accuracy:0.5292166666666667\n",
      "iteration76  cost:11.514888432768538, accuracy:0.5286833333333334\n",
      "iteration77  cost:11.212155019669472, accuracy:0.52805\n",
      "iteration78  cost:10.9193646853186, accuracy:0.5270166666666667\n",
      "iteration79  cost:10.636190452666177, accuracy:0.5252666666666667\n",
      "iteration80  cost:10.362316152410871, accuracy:0.5238499999999999\n",
      "iteration81  cost:10.097436061294125, accuracy:0.5223333333333333\n",
      "iteration82  cost:9.841254552789996, accuracy:0.5211833333333333\n",
      "iteration83  cost:9.593485759750195, accuracy:0.5194666666666667\n",
      "iteration84  cost:9.353853248580254, accuracy:0.5178\n",
      "iteration85  cost:9.122089704538475, accuracy:0.5164\n",
      "iteration86  cost:8.897936627764313, accuracy:0.51405\n",
      "iteration87  cost:8.681144039657386, accuracy:0.5117833333333333\n",
      "iteration88  cost:8.471470199242267, accuracy:0.5098833333333334\n",
      "iteration89  cost:8.26868132916755, accuracy:0.5079333333333333\n",
      "iteration90  cost:8.072551351000723, accuracy:0.5057\n",
      "iteration91  cost:7.882861629492661, accuracy:0.50315\n",
      "iteration92  cost:7.699400725497608, accuracy:0.5012\n",
      "iteration93  cost:7.521964157245957, accuracy:0.49955000000000005\n",
      "iteration94  cost:7.350354169678232, accuracy:0.49755000000000005\n",
      "iteration95  cost:7.18437951155925, accuracy:0.4957666666666667\n",
      "iteration96  cost:7.023855220101788, accuracy:0.49496666666666667\n",
      "iteration97  cost:6.868602412838811, accuracy:0.4933166666666666\n",
      "iteration98  cost:6.7184480864928755, accuracy:0.49191666666666667\n",
      "iteration99  cost:6.57322492260037, accuracy:0.4894833333333334\n",
      "iteration100  cost:6.43277109965709, accuracy:0.4875333333333334\n",
      "0.48573333333333335\n",
      "0.488\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def calculate_accuracy(model, X, Y):\n",
    "    m, n = X.shape\n",
    "    Y_pre = model.predict(X)\n",
    "    Y_pre = Y_pre == np.max(Y_pre, axis=0, keepdims = True)\n",
    "    C = np.abs(Y_pre - Y)\n",
    "    error = np.sum(C) / 2\n",
    "    print(1 - error / n)\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_train, Y_train = load_mnist(path=\"MNIST\")\n",
    "X_train = X_train.transpose()\n",
    "Y_train = convert_to_one_hot(Y_train, 10)\n",
    "m, n = X_train.shape\n",
    "# normalization fc-layer inputs\n",
    "nn_input_min = np.min(X_train, axis=1, keepdims=True)\n",
    "nn_input_max = np.max(X_train, axis=1, keepdims=True)\n",
    "nn_input_final = ((X_train - nn_input_min) / (nn_input_max - nn_input_min + 1e-11))\n",
    "X_train = nn_input_final\n",
    "\n",
    "linearLayer1 = FullyConnected(m, 40)\n",
    "linearLayer1.layer = 1\n",
    "activator1 = Activator('tanh')\n",
    "activator1.layer = 2\n",
    "linearLayer2 = FullyConnected(40, 20)\n",
    "linearLayer2.layer = 3\n",
    "activator2 = Activator('tanh')\n",
    "activator2.layer = 4\n",
    "linearLayer3 = FullyConnected(20, 10)\n",
    "linearLayer3.layer = 5\n",
    "# outputLayer = NNActivator(linearLayer3, 'softmax', 10)\n",
    "\n",
    "model = Sequence([linearLayer1, activator1, linearLayer2, activator2, linearLayer3], learning_rate=0.01, iteration_count=100, use_mini_batch=False, mini_batch_size=32)\n",
    "#optimizer = Adam()\n",
    "#optimizer = RMSProp()\n",
    "#optimizer = Momentum()\n",
    "#optimizer = AdaGrad()\n",
    "#optimizer.run(model, X_train, Y_train)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "X_test, Y_test = load_mnist(path=\"MNIST\", kind=\"t10k\")\n",
    "X_test = X_test.transpose()\n",
    "Y_test = convert_to_one_hot(Y_test, 10)\n",
    "\n",
    "calculate_accuracy(model, X_train, Y_train)\n",
    "calculate_accuracy(model, X_test, Y_test)\n",
    "\n",
    "print(\"over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
